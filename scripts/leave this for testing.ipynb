{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "209a991c-702f-45c4-90e0-f7f876253de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "from zipfile import ZipFile\n",
    "import zipfile\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from bs4 import BeautifulSoup\n",
    "from json import dump\n",
    "from urllib.request import urlopen\n",
    "\n",
    "########################################   population  ########################################\n",
    "dls = \"https://www.abs.gov.au/statistics/people/population/regional-population/2021/32180DS0001_2001-21.xlsx\"\n",
    "resp = requests.get(dls)\n",
    "output = open('../data/raw/population.xlsx', 'wb')\n",
    "output.write(resp.content)\n",
    "output.close()\n",
    "\n",
    "########################################   SA2  ########################################\n",
    "dls = \"https://www.abs.gov.au/statistics/standards/australian-statistical-geography-standard-asgs-edition-3/jul2021-jun2026/access-and-downloads/digital-boundary-files/SA2_2021_AUST_SHP_GDA2020.zip\"\n",
    "resp = requests.get(dls)\n",
    "# upload SA2.zip\n",
    "output = open('../data/raw/SA2.zip', 'wb')\n",
    "output.write(resp.content)\n",
    "output.close()\n",
    "#directory = \"data/raw/SA2\"\n",
    "# Parent Directory path\n",
    "# parent_dir = \"data/raw\"\n",
    "# = os.path.join(parent_dir, directory) \n",
    "path = \"../data/raw/SA2\"\n",
    "#create folder\n",
    "#os.mkdir(path) \n",
    "# save zip to a folder\n",
    "with zipfile.ZipFile(\"../data/raw/SA2.zip\", mode=\"r\") as archive:\n",
    "    archive.extractall(\"../data/raw/SA2\")\n",
    "    archive.close()\n",
    "# remove SA2.zip\n",
    "os.remove(\"../data/raw/SA2.zip\")\n",
    "\n",
    "########################################   ptv  ########################################\n",
    "dls = \"http://data.ptv.vic.gov.au/downloads/gtfs.zip\"\n",
    "resp = requests.get(dls)\n",
    "# upload gtfs.zip\n",
    "output = open('../data/raw/gtfs.zip', 'wb')\n",
    "output.write(resp.content)\n",
    "output.close()\n",
    "# directory = \"data/raw/ptv\"\n",
    "# Parent Directory path \n",
    "path = \"../data/raw/ptv\"\n",
    "# path = os.path.join(parent_dir, directory) \n",
    "#create folder\n",
    "os.mkdir(path) \n",
    "# save zip to a folder\n",
    "with zipfile.ZipFile(\"../data/raw/gtfs.zip\", mode=\"r\") as archive:\n",
    "    archive.extractall(\"../data/raw/ptv\")\n",
    "    archive.close()\n",
    "# remove gtfs.zip\n",
    "os.remove(\"../data/raw/gtfs.zip\")\n",
    "###ptv unzip 2\n",
    "with zipfile.ZipFile(\"../data/raw/ptv/2/google_transit.zip\", mode=\"r\") as archive:\n",
    "    archive.extractall(\"../data/raw/ptv/2\")\n",
    "    archive.close()\n",
    "# remove google_transit.zip\n",
    "os.remove(\"../data/raw/ptv/2/google_transit.zip\")\n",
    "###ptv unzip 3\n",
    "with zipfile.ZipFile(\"../data/raw/ptv/3/google_transit.zip\", mode=\"r\") as archive:\n",
    "    archive.extractall(\"../data/raw/ptv/3\")\n",
    "    archive.close()\n",
    "# remove google_transit.zip\n",
    "os.remove(\"../data/raw/ptv/3/google_transit.zip\")\n",
    "###ptv unzip 4\n",
    "with zipfile.ZipFile(\"../data/raw/ptv/4/google_transit.zip\", mode=\"r\") as archive:\n",
    "    archive.extractall(\"../data/raw/ptv/4\")\n",
    "    archive.close()\n",
    "# remove google_transit.zip\n",
    "os.remove(\"../data/raw/ptv/4/google_transit.zip\")\n",
    "\n",
    "########################################   school locations  ########################################\n",
    "dls = \"https://www.education.vic.gov.au/Documents/about/research/datavic/dv309_schoollocations2021.csv\"\n",
    "resp = requests.get(dls)\n",
    "output = open('../data/raw/school2021.csv', 'wb')\n",
    "output.write(resp.content)\n",
    "output.close()\n",
    "\n",
    "########################################   hospital locations  ########################################\n",
    "dls = \"https://data.humdata.org/dataset/a5221b34-8ed4-4e19-88c9-b195c13502b6/resource/6df0921e-d676-4c36-8229-c65cea510217/download/australia.csv\"\n",
    "resp = requests.get(dls)\n",
    "output = open('../data/raw/hospital2021.csv', 'wb')\n",
    "output.write(resp.content)\n",
    "output.close()\n",
    "\n",
    "########################################   income locations  ########################################\n",
    "dls = \"https://www.abs.gov.au/AUSSTATS/subscriber.nsf/log?openagent&14100do0004_2014-19.xlsx&1410.0&Data%20Cubes&63757E101C2DA1A1CA2586290010B831&0&2014-19&24.11.2020&Latest\"\n",
    "resp = requests.get(dls)\n",
    "output = open('../data/raw/income.xlsx', 'wb')\n",
    "output.write(resp.content)\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b78015d8-c1ff-4b2a-a663-9fe4361d65c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# built-in imports\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (X11; CrOS x86_64 12871.102.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.141 Safari/537.36\"}\n",
    "# constants\n",
    "BASE_URL = \"https://www.domain.com.au\"\n",
    "#N_PAGES = range(1, 51) # update this to your liking\n",
    "Postcode = range(3000, 4000)\n",
    "\n",
    "\n",
    "########################################  domain website scraping  ########################################\n",
    "# begin code\n",
    "url_links = []\n",
    "property_metadata = defaultdict(dict)\n",
    "# generate list of urls to visit\n",
    "for postcode in Postcode:\n",
    "    #url = BASE_URL + f\"/rent/melbourne-region-vic/?sort=price-desc&page={page}\"\n",
    "\n",
    "    url = BASE_URL + f\"/rent/?excludedeposittaken=1&postcode={postcode}\"\n",
    "    bs_object = BeautifulSoup(requests.get(\n",
    "        url, headers=headers).text, \"html.parser\")\n",
    "    # find the unordered list (ul) elements which are the results, then\n",
    "    # find all href (a) tags that are from the base_url website.\n",
    "    try:\n",
    "        index_links = bs_object \\\n",
    "        .find(\n",
    "            \"ul\",\n",
    "            {\"data-testid\": \"results\"}\n",
    "        ) \\\n",
    "        .findAll(\n",
    "            \"a\",\n",
    "            href=re.compile(f\"{BASE_URL}/*\") # the `*` denotes wildcard any\n",
    "        )\n",
    "        for link in index_links:\n",
    "        # if its a property address, add it to the list\n",
    "            if 'address' in link['class']:\n",
    "                url_links.append(link['href'])\n",
    "    except AttributeError:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "# for each url, scrape some basic metadata\n",
    "for property_url in url_links[1:]:\n",
    "    bs_object = BeautifulSoup(requests.get(\n",
    "    property_url, headers=headers).text, \"html.parser\")\n",
    "    # looks for the header class to get property name\n",
    "    property_metadata[property_url]['name'] = bs_object \\\n",
    "        .find(\"h1\", {\"class\": \"css-164r41r\"}) \\\n",
    "        .text\n",
    "    # looks for the div containing a summary title for cost\n",
    "    property_metadata[property_url]['cost_text'] = bs_object \\\n",
    "        .find(\"div\", {\"data-testid\": \"listing-details__summary-title\"}) \\\n",
    "        .text\n",
    "    # extract coordinates from the hyperlink provided\n",
    "    # i'll let you figure out what this does :P\n",
    "    property_metadata[property_url]['coordinates'] = [\n",
    "        float(coord) for coord in re.findall(\n",
    "            r'destination=([-\\s,\\d\\.]+)', # use regex101.com here if you need to\n",
    "            bs_object \\\n",
    "                .find(\n",
    "                    \"a\",\n",
    "                    {\"target\": \"_blank\", 'rel': \"noopener noreferer\"}\n",
    "                ) \\\n",
    "                .attrs['href']\n",
    "        )[0].split(',')\n",
    "    ]\n",
    "    rooms_list = []\n",
    "    for feature in bs_object \\\n",
    "            .find(\"div\", {\"data-testid\": \"property-features\"}) \\\n",
    "            .findAll(\"span\", {\"data-testid\": \"property-features-text-container\"}):\n",
    "        try:\n",
    "            rooms_list.append(re.findall(r'\\d\\s[A-Za-z]+', feature.text)[0])\n",
    "        except IndexError:\n",
    "            pass\n",
    "    property_metadata[property_url]['rooms'] = rooms_list\n",
    "    # property_metadata[property_url]['rooms'] = [\n",
    "    #     re.findall(r'\\d\\s[A-Za-z]+', feature.text)[0] for feature in bs_object \\\n",
    "    #         .find(\"div\", {\"data-testid\": \"property-features\"}) \\\n",
    "    #         .findAll(\"span\", {\"data-testid\": \"property-features-text-container\"})\n",
    "    # ]\n",
    "    property_metadata[property_url]['desc'] = re \\\n",
    "        .sub(r'<br\\/>', '\\n', str(bs_object.find(\"p\"))) \\\n",
    "        .strip('</p>')\n",
    "# output to example json in data/raw/\n",
    "with open('../data/raw/domain1.json', 'w') as f:\n",
    "    dump(property_metadata, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
