{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "924564e2-3937-4c60-a86c-c29048407de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "from zipfile import ZipFile\n",
    "import zipfile\n",
    "import json\n",
    "\n",
    "########################################   population  ########################################\n",
    "dls = \"https://www.abs.gov.au/statistics/people/population/regional-population/2021/32180DS0001_2001-21.xlsx\"\n",
    "resp = requests.get(dls)\n",
    "output = open('../data/raw/population.xlsx', 'wb')\n",
    "output.write(resp.content)\n",
    "output.close()\n",
    "\n",
    "########################################   SA2  ########################################\n",
    "dls = \"https://www.abs.gov.au/statistics/standards/australian-statistical-geography-standard-asgs-edition-3/jul2021-jun2026/access-and-downloads/digital-boundary-files/SA2_2021_AUST_SHP_GDA2020.zip\"\n",
    "resp = requests.get(dls)\n",
    "# upload SA2.zip\n",
    "output = open('../data/raw/SA2.zip', 'wb')\n",
    "output.write(resp.content)\n",
    "output.close()\n",
    "#directory = \"data/raw/SA2\"\n",
    "# Parent Directory path\n",
    "# parent_dir = \"data/raw\"\n",
    "# = os.path.join(parent_dir, directory) \n",
    "path = \"../data/raw/SA2\"\n",
    "#create folder\n",
    "#os.mkdir(path) \n",
    "# save zip to a folder\n",
    "with zipfile.ZipFile(\"../data/raw/SA2.zip\", mode=\"r\") as archive:\n",
    "    archive.extractall(\"../data/raw/SA2\")\n",
    "    archive.close()\n",
    "# remove SA2.zip\n",
    "os.remove(\"../data/raw/SA2.zip\")\n",
    "\n",
    "########################################   ptv  ########################################\n",
    "dls = \"http://data.ptv.vic.gov.au/downloads/gtfs.zip\"\n",
    "resp = requests.get(dls)\n",
    "# upload gtfs.zip\n",
    "output = open('../data/raw/gtfs.zip', 'wb')\n",
    "output.write(resp.content)\n",
    "output.close()\n",
    "# directory = \"data/raw/ptv\"\n",
    "# Parent Directory path \n",
    "path = \"../data/raw/ptv\"\n",
    "# path = os.path.join(parent_dir, directory) \n",
    "#create folder\n",
    "os.mkdir(path) \n",
    "# save zip to a folder\n",
    "with zipfile.ZipFile(\"../data/raw/gtfs.zip\", mode=\"r\") as archive:\n",
    "    archive.extractall(\"../data/raw/ptv\")\n",
    "    archive.close()\n",
    "# remove gtfs.zip\n",
    "os.remove(\"../data/raw/gtfs.zip\")\n",
    "###ptv unzip 2\n",
    "with zipfile.ZipFile(\"../data/raw/ptv/2/google_transit.zip\", mode=\"r\") as archive:\n",
    "    archive.extractall(\"../data/raw/ptv/2\")\n",
    "    archive.close()\n",
    "# remove google_transit.zip\n",
    "os.remove(\"../data/raw/ptv/2/google_transit.zip\")\n",
    "###ptv unzip 3\n",
    "with zipfile.ZipFile(\"../data/raw/ptv/3/google_transit.zip\", mode=\"r\") as archive:\n",
    "    archive.extractall(\"../data/raw/ptv/3\")\n",
    "    archive.close()\n",
    "# remove google_transit.zip\n",
    "os.remove(\"../data/raw/ptv/3/google_transit.zip\")\n",
    "###ptv unzip 4\n",
    "with zipfile.ZipFile(\"../data/raw/ptv/4/google_transit.zip\", mode=\"r\") as archive:\n",
    "    archive.extractall(\"../data/raw/ptv/4\")\n",
    "    archive.close()\n",
    "# remove google_transit.zip\n",
    "os.remove(\"../data/raw/ptv/4/google_transit.zip\")\n",
    "\n",
    "########################################   school locations  ########################################\n",
    "dls = \"https://www.education.vic.gov.au/Documents/about/research/datavic/dv309_schoollocations2021.csv\"\n",
    "resp = requests.get(dls)\n",
    "output = open('../data/raw/school2021.csv', 'wb')\n",
    "output.write(resp.content)\n",
    "output.close()\n",
    "\n",
    "########################################   hospital locations  ########################################\n",
    "dls = \"https://data.humdata.org/dataset/a5221b34-8ed4-4e19-88c9-b195c13502b6/resource/6df0921e-d676-4c36-8229-c65cea510217/download/australia.csv\"\n",
    "resp = requests.get(dls)\n",
    "output = open('../data/raw/hospital2021.csv', 'wb')\n",
    "output.write(resp.content)\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcb1300a-3010-4da7-87bd-2acb3e8edb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################   hospital locations  ########################################\n",
    "dls = \"https://data.humdata.org/dataset/a5221b34-8ed4-4e19-88c9-b195c13502b6/resource/6df0921e-d676-4c36-8229-c65cea510217/download/australia.csv\"\n",
    "resp = requests.get(dls)\n",
    "output = open('../data/raw/hospital2021.csv', 'wb')\n",
    "output.write(resp.content)\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "40e1c7ad-8dc0-46c2-8be3-8d96ba1b4d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A very simple and basic web scraping script. Feel free to\n",
    "use this as a source of inspiration, but, make sure to attribute\n",
    "it if you do so.\n",
    "\n",
    "This is by no means production code.\n",
    "\"\"\"\n",
    "# built-in imports\n",
    "import re\n",
    "from json import dump\n",
    "import requests\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "# user packages\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (X11; CrOS x86_64 12871.102.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.141 Safari/537.36\"}\n",
    "# constants\n",
    "BASE_URL = \"https://www.domain.com.au\"\n",
    "N_PAGES = range(1, 51) # update this to your liking\n",
    "\n",
    "# begin code\n",
    "url_links = []\n",
    "property_metadata = defaultdict(dict)\n",
    "\n",
    "# generate list of urls to visit\n",
    "for page in N_PAGES:\n",
    "    url = BASE_URL + f\"/rent/melbourne-region-vic/?sort=price-desc&page={page}\"\n",
    "    bs_object = BeautifulSoup(requests.get(\n",
    "        url, headers=headers).text, \"html.parser\")\n",
    "\n",
    "    # find the unordered list (ul) elements which are the results, then\n",
    "    # find all href (a) tags that are from the base_url website.\n",
    "    index_links = bs_object \\\n",
    "        .find(\n",
    "            \"ul\",\n",
    "            {\"data-testid\": \"results\"}\n",
    "        ) \\\n",
    "        .findAll(\n",
    "            \"a\",\n",
    "            href=re.compile(f\"{BASE_URL}/*\") # the `*` denotes wildcard any\n",
    "        )\n",
    "\n",
    "    for link in index_links:\n",
    "        # if its a property address, add it to the list\n",
    "        if 'address' in link['class']:\n",
    "            url_links.append(link['href'])\n",
    "\n",
    "\n",
    "# for each url, scrape some basic metadata\n",
    "for property_url in url_links[1:]:\n",
    "    bs_object = BeautifulSoup(requests.get(\n",
    "    property_url, headers=headers).text, \"html.parser\")\n",
    "    # looks for the header class to get property name\n",
    "    property_metadata[property_url]['name'] = bs_object \\\n",
    "        .find(\"h1\", {\"class\": \"css-164r41r\"}) \\\n",
    "        .text\n",
    "\n",
    "    # looks for the div containing a summary title for cost\n",
    "    property_metadata[property_url]['cost_text'] = bs_object \\\n",
    "        .find(\"div\", {\"data-testid\": \"listing-details__summary-title\"}) \\\n",
    "        .text\n",
    "\n",
    "    # extract coordinates from the hyperlink provided\n",
    "    # i'll let you figure out what this does :P\n",
    "    property_metadata[property_url]['coordinates'] = [\n",
    "        float(coord) for coord in re.findall(\n",
    "            r'destination=([-\\s,\\d\\.]+)', # use regex101.com here if you need to\n",
    "            bs_object \\\n",
    "                .find(\n",
    "                    \"a\",\n",
    "                    {\"target\": \"_blank\", 'rel': \"noopener noreferer\"}\n",
    "                ) \\\n",
    "                .attrs['href']\n",
    "        )[0].split(',')\n",
    "    ]\n",
    "\n",
    "    rooms_list = []\n",
    "    for feature in bs_object \\\n",
    "            .find(\"div\", {\"data-testid\": \"property-features\"}) \\\n",
    "            .findAll(\"span\", {\"data-testid\": \"property-features-text-container\"}):\n",
    "        try:\n",
    "            rooms_list.append(re.findall(r'\\d\\s[A-Za-z]+', feature.text)[0])\n",
    "        except IndexError:\n",
    "            pass\n",
    "\n",
    "    property_metadata[property_url]['rooms'] = rooms_list\n",
    "\n",
    "    # property_metadata[property_url]['rooms'] = [\n",
    "    #     re.findall(r'\\d\\s[A-Za-z]+', feature.text)[0] for feature in bs_object \\\n",
    "    #         .find(\"div\", {\"data-testid\": \"property-features\"}) \\\n",
    "    #         .findAll(\"span\", {\"data-testid\": \"property-features-text-container\"})\n",
    "    # ]\n",
    "\n",
    "    property_metadata[property_url]['desc'] = re \\\n",
    "        .sub(r'<br\\/>', '\\n', str(bs_object.find(\"p\"))) \\\n",
    "        .strip('</p>')\n",
    "\n",
    "# output to example json in data/raw/\n",
    "with open('../data/raw/example.json', 'w') as f:\n",
    "    dump(property_metadata, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb531dcb-5b7f-4814-bf16-a2017e42635b",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/raw/example.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-de3452c02eaa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/raw/example.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproperty_metadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/raw/example.json'"
     ]
    }
   ],
   "source": [
    "with open('data/raw/example.json', 'w') as f:\n",
    "    dump(property_metadata, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629c6556-16f0-4b7d-a8a5-378536e1688f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "import re\n",
    "import csv\n",
    "from zipfile import ZipFile\n",
    "import json\n",
    "\n",
    "# built-in imports\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (X11; CrOS x86_64 12871.102.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.141 Safari/537.36\"}\n",
    "# constants\n",
    "BASE_URL = \"https://www.domain.com.au\"\n",
    "N_PAGES = range(1, 51) # update this to your liking\n",
    "\n",
    "########################################  domain website scraping  ########################################\n",
    "# begin code\n",
    "url_links = []\n",
    "property_metadata = defaultdict(dict)\n",
    "# generate list of urls to visit\n",
    "for page in N_PAGES:\n",
    "    url = BASE_URL + f\"/rent/melbourne-region-vic/?sort=price-desc&page={page}\"\n",
    "    bs_object = BeautifulSoup(requests.get(\n",
    "        url, headers=headers).text, \"html.parser\")\n",
    "    # find the unordered list (ul) elements which are the results, then\n",
    "    # find all href (a) tags that are from the base_url website.\n",
    "    index_links = bs_object \\\n",
    "        .find(\n",
    "            \"ul\",\n",
    "            {\"data-testid\": \"results\"}\n",
    "        ) \\\n",
    "        .findAll(\n",
    "            \"a\",\n",
    "            href=re.compile(f\"{BASE_URL}/*\") # the `*` denotes wildcard any\n",
    "        )\n",
    "    for link in index_links:\n",
    "        # if its a property address, add it to the list\n",
    "        if 'address' in link['class']:\n",
    "            url_links.append(link['href'])\n",
    "# for each url, scrape some basic metadata\n",
    "for property_url in url_links[1:]:\n",
    "    bs_object = BeautifulSoup(requests.get(\n",
    "    property_url, headers=headers).text, \"html.parser\")\n",
    "    # looks for the header class to get property name\n",
    "    property_metadata[property_url]['name'] = bs_object \\\n",
    "        .find(\"h1\", {\"class\": \"css-164r41r\"}) \\\n",
    "        .text\n",
    "    # looks for the div containing a summary title for cost\n",
    "    property_metadata[property_url]['cost_text'] = bs_object \\\n",
    "        .find(\"div\", {\"data-testid\": \"listing-details__summary-title\"}) \\\n",
    "        .text\n",
    "    # extract coordinates from the hyperlink provided\n",
    "    # i'll let you figure out what this does :P\n",
    "    property_metadata[property_url]['coordinates'] = [\n",
    "        float(coord) for coord in re.findall(\n",
    "            r'destination=([-\\s,\\d\\.]+)', # use regex101.com here if you need to\n",
    "            bs_object \\\n",
    "                .find(\n",
    "                    \"a\",\n",
    "                    {\"target\": \"_blank\", 'rel': \"noopener noreferer\"}\n",
    "                ) \\\n",
    "                .attrs['href']\n",
    "        )[0].split(',')\n",
    "    ]\n",
    "    rooms_list = []\n",
    "    for feature in bs_object \\\n",
    "            .find(\"div\", {\"data-testid\": \"property-features\"}) \\\n",
    "            .findAll(\"span\", {\"data-testid\": \"property-features-text-container\"}):\n",
    "        try:\n",
    "            rooms_list.append(re.findall(r'\\d\\s[A-Za-z]+', feature.text)[0])\n",
    "        except IndexError:\n",
    "            pass\n",
    "    property_metadata[property_url]['rooms'] = rooms_list\n",
    "    # property_metadata[property_url]['rooms'] = [\n",
    "    #     re.findall(r'\\d\\s[A-Za-z]+', feature.text)[0] for feature in bs_object \\\n",
    "    #         .find(\"div\", {\"data-testid\": \"property-features\"}) \\\n",
    "    #         .findAll(\"span\", {\"data-testid\": \"property-features-text-container\"})\n",
    "    # ]\n",
    "    property_metadata[property_url]['desc'] = re \\\n",
    "        .sub(r'<br\\/>', '\\n', str(bs_object.find(\"p\"))) \\\n",
    "        .strip('</p>')\n",
    "# output to example json in data/raw/\n",
    "with open('../data/raw/example.json', 'w') as f:\n",
    "    dump(property_metadata, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66b83169-3842-4ee6-aea8-4078c5d9a34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = \"http://data.ptv.vic.gov.au/downloads/gtfs.zip\"\n",
    "resp = requests.get(dls)\n",
    "# upload gtfs.zip\n",
    "output = open('../data/raw/gtfs.zip', 'wb')\n",
    "output.write(resp.content)\n",
    "output.close()\n",
    "# directory = \"data/raw/ptv\"\n",
    "# Parent Directory path \n",
    "path = \"../data/raw/ptv\"\n",
    "# path = os.path.join(parent_dir, directory) \n",
    "#create folder\n",
    "os.mkdir(path) \n",
    "# save zip to a folder\n",
    "with zipfile.ZipFile(\"../data/raw/gtfs.zip\", mode=\"r\") as archive:\n",
    "    archive.extractall(\"../data/raw/ptv\")\n",
    "    archive.close()\n",
    "# remove gtfs.zip\n",
    "os.remove(\"../data/raw/gtfs.zip\")\n",
    "###ptv unzip 2\n",
    "with zipfile.ZipFile(\"../data/raw/ptv/2/google_transit.zip\", mode=\"r\") as archive:\n",
    "    archive.extractall(\"../data/raw/ptv/2\")\n",
    "    archive.close()\n",
    "# remove google_transit.zip\n",
    "os.remove(\"../data/raw/ptv/2/google_transit.zip\")\n",
    "###ptv unzip 3\n",
    "with zipfile.ZipFile(\"../data/raw/ptv/3/google_transit.zip\", mode=\"r\") as archive:\n",
    "    archive.extractall(\"../data/raw/ptv/3\")\n",
    "    archive.close()\n",
    "# remove google_transit.zip\n",
    "os.remove(\"../data/raw/ptv/3/google_transit.zip\")\n",
    "###ptv unzip 4\n",
    "with zipfile.ZipFile(\"../data/raw/ptv/4/google_transit.zip\", mode=\"r\") as archive:\n",
    "    archive.extractall(\"../data/raw/ptv/4\")\n",
    "    archive.close()\n",
    "# remove google_transit.zip\n",
    "os.remove(\"../data/raw/ptv/4/google_transit.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209a991c-702f-45c4-90e0-f7f876253de4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
